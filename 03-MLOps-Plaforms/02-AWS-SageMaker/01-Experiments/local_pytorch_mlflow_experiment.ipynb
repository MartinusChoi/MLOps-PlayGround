{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf486f78",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af78b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7cc1e6",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3ff34fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.73MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 152kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.41MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.02MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_set = datasets.MNIST(\n",
    "    \"mnist_data\",\n",
    "    train=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,),(0.3081,))\n",
    "    ]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_set = datasets.MNIST(\n",
    "    \"mnist_data\",\n",
    "    train=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,),(0.3081,))\n",
    "    ]),\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44c04a",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7912e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, hidden_channels, kernel_size, dropout):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(1, hidden_channels[0], kernel_size=kernel_size)\n",
    "\n",
    "        self.conv_2 = nn.Conv2d(hidden_channels[0], 20, kernel_size=kernel_size)\n",
    "\n",
    "        self.conv_2_drop = nn.Dropout2d(p=dropout)\n",
    "\n",
    "        self.maxpool = F.max_pool2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.dropout = F.dropout()\n",
    "\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "\n",
    "        self.activation = F.relu()\n",
    "\n",
    "        self.log_softmax = F.log_softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.maxpool(x, 2)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.conv_2(x)\n",
    "        x = self.conv_2_drop(x)\n",
    "        x = self.maxpool(x, 2)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        output = self.log_softmax(x, dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e153a",
   "metadata": {},
   "source": [
    "# Define Performance Logging Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e3271d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_performance(model, data_loader, device, epoch, metric_type=\"Test\"):\n",
    "    model.eval()\n",
    "\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(data_loader, total=len(data_loader), desc=f\"Log {metric_type} Perfomance >\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss += F.nll_loss(\n",
    "                output, target,\n",
    "                reduction=\"sum\"\n",
    "            ).item()\n",
    "\n",
    "            prediction = output.max(1, keepdim=True)[1]\n",
    "            correct += prediction.eq(target.view_as(prediction)).sum().item()\n",
    "\n",
    "        loss /= len(data_loader.dataset)\n",
    "        accuracy = 100.0 * correct / len(data_loader.dataset)\n",
    "    \n",
    "    mlflow.log_metric(f\"{metric_type}_loss\", loss, step=epoch)\n",
    "    mlflow.log_metric(f\"{metric_type}_accuracy\", accuracy, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513c15dc",
   "metadata": {},
   "source": [
    "# Define Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0559ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        train_set,\n",
    "        test_set,\n",
    "        param_set,\n",
    "        experiment_name,\n",
    "        run_name,\n",
    "        optimizer=\"sgd\"\n",
    "):\n",
    "    mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.pytorch.autolog()\n",
    "\n",
    "        mlflow.log_param('batch_size', param_set['batch_size'])\n",
    "        mlflow.log_param('hidden_channels', param_set['hidden_chennels'])\n",
    "        mlflow.log_param('kernel_size', param_set['kernel_size'])\n",
    "        mlflow.log_param('dropout', param_set['dropout'])\n",
    "        mlflow.log_param('momentum', param_set['momentum'])\n",
    "        mlflow.log_param('learning_rate', param_set['learning_rate'])\n",
    "        mlflow.log_param('epochs', param_set['epochs'])\n",
    "        mlflow.log_param('optimizer', optimizer)\n",
    "\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        train_loader = DataLoader(train_set, batch_size=param_set['batch_size'], shuffle=True)\n",
    "        test_loader = DataLoader(test_set, batch_size=1000, shuffle=True)\n",
    "\n",
    "        model = MNISTClassifier(\n",
    "            num_classes=10,\n",
    "            \n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
